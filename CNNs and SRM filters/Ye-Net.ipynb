{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ye-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from keras.layers import Activation\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow.keras.layers import Lambda, Layer, ReLU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, LSTM, SpatialDropout2D, Concatenate\n",
    "tf.keras.layers.Concatenate()\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, UpSampling2D, BatchNormalization\n",
    "from keras.layers.core import Reshape\n",
    "from keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from keras import Input, Model\n",
    "from time import time\n",
    "import time as tm\n",
    "from keras.initializers import Constant, RandomNormal, glorot_normal\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.layers import  concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30 SRM filters for preprocessing and the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################## 30 SRM FILTERS\n",
    "srm_weights = np.load('SRM_Kernels.npy') \n",
    "biasSRM=numpy.ones(30)\n",
    "print (srm_weights.shape)\n",
    "################################################## TLU ACTIVATION FUNCTION\n",
    "T3 = 3;\n",
    "def Tanh3(x):\n",
    "    tanh3 = K.tanh(x)*T3\n",
    "    return tanh3\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ye-Net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ye_Net(img_size=256):\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    #Inputs\n",
    "    inputs = tf.keras.Input(shape=(img_size,img_size,1), name=\"input_1\")\n",
    "    print(inputs.shape)\n",
    "    \n",
    "    #Block 1\n",
    "    layers = tf.keras.layers.Conv2D(30, (5,5), weights=[srm_weights,biasSRM], strides=(1,1), trainable=False, activation=Tanh3, use_bias=True)(inputs)\n",
    "    #layers = tf.keras.layers.Lambda(tf.keras.backend.abs)(layers)\n",
    "    #layer1 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    print(layers.shape)\n",
    "    \n",
    "    #Block 2\n",
    "    \n",
    "    #layers = tf.keras.layers.SpatialDropout2D(rate=0.1)(layers)\n",
    "    \n",
    "    layers = tf.keras.layers.Conv2D(30, (3,3), strides=(1,1), kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers) \n",
    "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers = tf.keras.layers.Lambda(tf.keras.backend.abs)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    layers = tf.keras.layers.Concatenate()([layers, layers, layers])\n",
    "    print(layers.shape)\n",
    "    \n",
    "    #Block 3\n",
    "    layers = tf.keras.layers.SpatialDropout2D(rate=0.1)(layers)\n",
    "    layers = tf.keras.layers.Conv2D(30, (3,3), strides=(1,1), kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers) \n",
    "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers = tf.keras.layers.Lambda(tf.keras.backend.abs)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    print(layers.shape)\n",
    "    \n",
    "    #Block 4\n",
    "    layers = tf.keras.layers.SpatialDropout2D(rate=0.1)(layers)\n",
    "    layers = tf.keras.layers.Conv2D(30, (3,3), strides=(1,1), kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers) \n",
    "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers = tf.keras.layers.Lambda(tf.keras.backend.abs)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    layers = tf.keras.layers.AveragePooling2D((2,2), strides= (2,2))(layers)\n",
    "    print(layers.shape)\n",
    "    \n",
    "    #Block 5\n",
    "    layers = tf.keras.layers.SpatialDropout2D(rate=0.1)(layers)\n",
    "    layers = tf.keras.layers.Conv2D(32, (5,5), strides=(1,1), kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers)\n",
    "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers = tf.keras.layers.Lambda(tf.keras.backend.abs)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    print(layers.shape)\n",
    "    \n",
    "    #Block 6\n",
    "    layers = tf.keras.layers.Concatenate()([layers, layers, layers])\n",
    "    layers = tf.keras.layers.GlobalAveragePooling2D(data_format=\"channels_last\")(layers)\n",
    "    layers = tf.keras.layers.Dense(128,kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers)\n",
    "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    #layers = tf.keras.layers.Dropout(0.2)(layers)\n",
    "    layers = tf.keras.layers.Dense(64,kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers)\n",
    "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    #layers = tf.keras.layers.Dropout(0.2)(layers)\n",
    "    layers = tf.keras.layers.Dense(32,kernel_initializer='glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers)\n",
    "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    predictions = tf.keras.layers.Dense(2,kernel_initializer='glorot_normal', activation=\"softmax\", name=\"output_1\",kernel_regularizer=tf.keras.regularizers.l2(0.0001),bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers)\n",
    "    print(predictions.shape)\n",
    "    \n",
    "    #Model generation\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=predictions)\n",
    "    #Optimizer\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.95)#lrate\n",
    "    #Compilator\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print (\"Ye-net model 2 generated\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining different functions to work with the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size, epochs, initial_epoch = 0, model_name=\"\"):\n",
    "    start_time = tm.time()\n",
    "    log_dir=\"/content/drive/My Drive/Colab Notebooks/Steganalysis/logs/\"+model_name+\"_\"+\"{}\".format(time())\n",
    "    tensorboard = tf.keras.callbacks.TensorBoard(log_dir)\n",
    "    filepath = log_dir+\"/saved-model-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', save_best_only=False, mode='max')\n",
    "    model.reset_states()\n",
    "    history=model.fit(X_train, y_train, epochs=epochs, \n",
    "                        callbacks=[tensorboard,checkpoint], \n",
    "                        batch_size=batch_size,validation_data=(X_valid, y_valid),initial_epoch=initial_epoch)\n",
    "    \n",
    "    metrics = model.evaluate(X_test, y_test, verbose=0)\n",
    "    results_dir=\"/content/drive/My Drive/Colab Notebooks/Steganalysis/Results/\"+model_name+\"/\"\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "      \n",
    "    with plt.style.context('seaborn-white'):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        #plt.subplot(1,2,1)\n",
    "        #Plot training & validation accuracy values\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('Accuracy Vs Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.grid('on')\n",
    "        plt.savefig(results_dir+'Accuracy_Ye_Net_'+model_name+'.eps', format='eps')\n",
    "        plt.savefig(results_dir+'Accuracy_Ye_Net_'+model_name+'.svg', format='svg')\n",
    "        plt.savefig(results_dir+'Accuracy_Ye_Net_'+model_name+'.pdf', format='pdf')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        #plt.subplot(1,2,2)\n",
    "        #Plot training & validation loss values\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Loss Vs Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.grid('on')\n",
    "        plt.savefig(results_dir+'Loss_Ye_Net_'+model_name+'.eps', format='eps')\n",
    "        plt.savefig(results_dir+'Loss_Ye_Net_'+model_name+'.svg', format='svg')\n",
    "        plt.savefig(results_dir+'Loss_Ye_Net_'+model_name+'.pdf', format='pdf')\n",
    "        plt.show()\n",
    "\n",
    "        '''\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        #plt.subplot(1,2,2)\n",
    "        #Plot training & validation loss values\n",
    "        plt.plot(history.history['lr'])\n",
    "        plt.ylabel('Lr')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.grid('on')\n",
    "        plt.show()\n",
    "        '''\n",
    "    TIME = tm.time() - start_time\n",
    "    print(\"Time \"+model_name+\" = %s [seconds]\" % TIME)\n",
    "    return {k:v for k,v in zip (model.metrics_names, metrics)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Final_Results_Test(model,PATH_trained_models):\n",
    "    B_accuracy = 0 #B --> Best\n",
    "    for filename in os.listdir(PATH_trained_models):\n",
    "        if filename != ('train') and filename != ('validation'):\n",
    "            print(filename)\n",
    "            model.load_weights(PATH_trained_models+'/'+filename)\n",
    "            loss,accuracy = model.evaluate(X_test, y_test,verbose=0)\n",
    "            print(f'Loss={loss:.4f} y Accuracy={accuracy:0.4f}'+'\\n') \n",
    "            if accuracy > B_accuracy:\n",
    "                B_accuracy = accuracy\n",
    "                B_loss = loss\n",
    "                B_name = filename\n",
    "    print(\"\\n\\nBest\")\n",
    "    print(B_name)\n",
    "    print(f'Loss={B_loss:.4f} y Accuracy={B_accuracy:0.4f}'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def plot_train_valid(model,PATH_trained_models,model_name):\n",
    "    acc_train=[]\n",
    "    acc_valid=[]\n",
    "    loss_train=[]\n",
    "    loss_valid=[]\n",
    "    for filename in tqdm(os.listdir(PATH_trained_models)):\n",
    "        if filename != ('train') and filename != ('validation'):\n",
    "            print(filename)\n",
    "            model.load_weights(PATH_trained_models+'/'+filename)\n",
    "            loss,accuracy = model.evaluate(X_train, y_train,verbose=0)\n",
    "            acc_train.append(accuracy)\n",
    "            loss_train.append(loss)\n",
    "            loss,accuracy = model.evaluate(X_valid, y_valid,verbose=0)\n",
    "            acc_valid.append(accuracy)\n",
    "            loss_valid.append(loss)\n",
    "\n",
    "    results_dir=\"/content/drive/My Drive/Colab Notebooks/Steganalysis/Results/\"+model_name+\"/\"\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "\n",
    "    with plt.style.context('seaborn-white'):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        #plt.subplot(1,2,1)\n",
    "        #Plot training & validation accuracy values\n",
    "        plt.plot(acc_train)\n",
    "        plt.plot(acc_valid)\n",
    "        plt.title('Accuracy Vs Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.grid('on')\n",
    "        plt.savefig(results_dir+'Accuracy_Ye_Net_'+model_name+'.eps', format='eps')\n",
    "        plt.savefig(results_dir+'Accuracy_Ye_Net_'+model_name+'.svg', format='svg')\n",
    "        plt.savefig(results_dir+'Accuracy_Ye_Net_'+model_name+'.pdf', format='pdf')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        #plt.subplot(1,2,2)\n",
    "        #Plot training & validation loss values\n",
    "        plt.plot(loss_train)\n",
    "        plt.plot(loss_valid)\n",
    "        plt.title('Loss Vs Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.grid('on')\n",
    "        plt.savefig(results_dir+'Loss_Ye_Net_'+model_name+'.eps', format='eps')\n",
    "        plt.savefig(results_dir+'Loss_Ye_Net_'+model_name+'.svg', format='svg')\n",
    "        plt.savefig(results_dir+'Loss_Ye_Net_'+model_name+'.pdf', format='pdf')\n",
    "        plt.show()\n",
    "\n",
    "        '''\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        #plt.subplot(1,2,2)\n",
    "        #Plot training & validation loss values\n",
    "        plt.plot(history.history['lr'])\n",
    "        plt.ylabel('Lr')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.grid('on')\n",
    "        plt.show()\n",
    "        '''\n",
    "    results={'acc_train':acc_train,'acc_valid':acc_valid,'loss_train':loss_train,'loss_valid':loss_valid}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "\n",
    "def get_curve(gt, pred, target_names,model_name):\n",
    "    labels=[]\n",
    "    for i in range(len(target_names)):\n",
    "        \n",
    "        curve_function = roc_curve\n",
    "        auc_roc = roc_auc_score(gt[:, i], pred[:, i])\n",
    "        label = model_name+target_names[i] + \" AUC: %.3f \" % auc_roc\n",
    "        labels.append(label)\n",
    "        xlabel = \"False positive rate\"\n",
    "        ylabel = \"True positive rate\"\n",
    "        a, b, _ = curve_function(gt[:, i], pred[:, i])\n",
    "        plt.figure(1, figsize=(7, 7))\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.plot(a, b, label=label)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "\n",
    "        plt.legend(loc='upper center', bbox_to_anchor=(1.3, 1),\n",
    "                  fancybox=True, ncol=1)\n",
    "      \n",
    "    return [a,b],labels\n",
    "labels = [\"Cover\",\"Stego\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with BOSSbase 1.01 WOW y PAYLOAD = 0.4bpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the README, there is a link to download the databases we use for the work. There are two databases BOSSbase 1.01 and Bows2+ BOSSbase 1.01, cover images and stego. You can choose the payload of 0.2 and 0.4 bpp for each of the databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH04 = 'drive/My Drive/Databases/Payload_04bpp'\n",
    "#Dataset\n",
    "EPOCHS=100\n",
    "PATH04_WOW1 = \"/WOW_04bpp_BOSS/\"\n",
    "\n",
    "#Train\n",
    "X_train = np.load(PATH04+PATH04_WOW1+'X_train.npy')\n",
    "y_train = np.load(PATH04+PATH04_WOW1+'y_train.npy')\n",
    "#Valid\n",
    "X_valid = np.load(PATH04+PATH04_WOW1+'X_valid.npy')\n",
    "y_valid = np.load(PATH04+PATH04_WOW1+'y_valid.npy')\n",
    "#Test\n",
    "X_test = np.load(PATH04+PATH04_WOW1+'X_test.npy')\n",
    "y_test = np.load(PATH04+PATH04_WOW1+'y_test.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN name and algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name=\"04WOW1\"\n",
    "m_name=\"Ye_Net\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Ye_Net() \n",
    "name=\"Model_\"+m_name+\"_\"+base_name\n",
    "_, history  = train(model, X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size=64, epochs=100, model_name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Ye_Net() \n",
    "PATH_trained_models = \" \"\n",
    "Final_Results_Test(model,PATH_trained_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, validation and testing graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Ye_Net()  \n",
    "PATH_trained_models = \"\"\n",
    "name=\"Model_\"+m_name+\"_\"+base_name\n",
    "res=plot_train_valid(model,PATH_trained_models,name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Ye_Net() \n",
    "model.load_weights(\"\") #path best model\n",
    "predictions= model.predict(X_test,verbose=0)\n",
    "labels = [\"Cover\",\"Stego\"]\n",
    "model_name=\"Ye_Net WOW - \"\n",
    "curve1,labels1=get_curve(y_test, predictions, labels,model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to train the algorithm with another database, change \"PATH04_WOW1\" and  \"base_name\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when you train a model with a 0.2bpp payload, you need to transfer the best model at 0.4bpp. For example, if I need to train the CNN with WOW 0.2bpp, you need to transfer the weights of the best model trained on WOW 0.4bpp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example train CNN with WOW 0.2bpp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name=\"02WOW1\"\n",
    "m_name=\"Ye_Net\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Ye_Net() \n",
    "model.load_weights(\"\") #best Model 0.4\n",
    "name=\"Model_\"+m_name+\"_\"+base_name\n",
    "_, history  = train(model, X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size=64, epochs=50, model_name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Ye_Net() \n",
    "PATH_trained_models = \" \"\n",
    "Final_Results_Test(model,PATH_trained_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, validation and testing graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Ye_Net()  \n",
    "PATH_trained_models = \"\"\n",
    "name=\"Model_\"+m_name+\"_\"+base_name\n",
    "res=plot_train_valid(model,PATH_trained_models,name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Ye_Net() \n",
    "model.load_weights(\"\") #best Model train\n",
    "predictions= model.predict(X_test,verbose=0)\n",
    "labels = [\"Cover\",\"Stego\"]\n",
    "model_name=\"Ye-Net WOW - \"\n",
    "curve3_1,labels3_1=get_curve(y_test, predictions, labels,model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Semillero",
   "language": "python",
   "name": "semillero"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
